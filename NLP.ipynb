{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe2aa090-288c-4e3c-8854-aab65fd04104",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import syllables\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords as nltk_sw\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2e52e7c-a186-48b8-8487-ea0f4badcd97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['https://insights.blackcoffer.com/rise-of-telemedicine-and-its-impact-on-livelihood-by-2040-3-2/',\n",
       "  'https://insights.blackcoffer.com/rise-of-e-health-and-its-impact-on-humans-by-the-year-2030/',\n",
       "  'https://insights.blackcoffer.com/rise-of-e-health-and-its-imapct-on-humans-by-the-year-2030-2/',\n",
       "  'https://insights.blackcoffer.com/rise-of-telemedicine-and-its-impact-on-livelihood-by-2040-2/',\n",
       "  'https://insights.blackcoffer.com/rise-of-telemedicine-and-its-impact-on-livelihood-by-2040-2-2/',\n",
       "  'https://insights.blackcoffer.com/rise-of-chatbots-and-its-impact-on-customer-support-by-the-year-2040/',\n",
       "  'https://insights.blackcoffer.com/rise-of-e-health-and-its-imapct-on-humans-by-the-year-2030/',\n",
       "  'https://insights.blackcoffer.com/how-does-marketing-influence-businesses-and-consumers/',\n",
       "  'https://insights.blackcoffer.com/how-advertisement-increase-your-market-value/',\n",
       "  'https://insights.blackcoffer.com/negative-effects-of-marketing-on-society/'],\n",
       " 114)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"Input.xlsx\")\n",
    "urls = list(df[\"URL\"])\n",
    "urls[:10], len(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096584fc-3007-4b37-b77e-1f9f9abe6a72",
   "metadata": {},
   "source": [
    "# fetch_web_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5368e735-c04a-447c-9cd6-ec8e66780f92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oil prices by the year 2040, and how it will impact the world economy.\\n We are in an interconnected world. Any change in one part of the world will always lead to some #changes in other parts of the world as well, maybe a bit later but surely there will be some change and that is what we are seeing in today’s world. Electric vehicles are the change that we are seeing in today’s world. With so many advancements in technology, economies are getting bigger. China might surpass the US and become a #'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fetch_web_data(url):\n",
    "    class_ = [\"td-post-content tagdiv-type\", \"tdb-block-inner td-fix-index\"]\n",
    "    doc = requests.get(url)\n",
    "    soup = BeautifulSoup(doc.content, \"html.parser\")\n",
    "    title = soup.find(\"h1\")\n",
    "    article = soup.find_all(\"div\", {\"class\": class_[0]})\n",
    "    if article:\n",
    "        res = \" \"\n",
    "        for tag in article:\n",
    "            res += tag.text.strip()\n",
    "    else:\n",
    "        article = soup.find_all(\"div\", {\"class\": class_[1]})\n",
    "        res = \" \"\n",
    "        for tag in article:\n",
    "            res += tag.text.strip()\n",
    "    try:\n",
    "        start = res.index(\"Introduction\")\n",
    "        stop = res.index(\"Blackcoffer Insights\")\n",
    "    except:\n",
    "        start = 0\n",
    "        stop = -1\n",
    "    return title.text + \"\\n\" + res[start:stop]\n",
    "\n",
    "\n",
    "fetch_web_data(random.choice(urls))[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b449e93b-b3b8-4f3e-865c-b09cb4cfaf79",
   "metadata": {},
   "source": [
    "# Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "906f60f3-9387-4b4e-bb23-7f3db58bc35d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ERNST',\n",
       " 'YOUNG',\n",
       " 'DELOITTE',\n",
       " 'TOUCHE',\n",
       " 'KPMG',\n",
       " 'PRICEWATERHOUSECOOPERS',\n",
       " 'PRICEWATERHOUSE',\n",
       " 'COOPERS',\n",
       " 'AFGHANI',\n",
       " 'ARIARY']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_stop_words():\n",
    "    StopWords_notNames = []\n",
    "    for file in os.listdir(\"StopWords/\"):\n",
    "        if file != \"StopWords_Names.txt\":\n",
    "            corpus = open(f\"StopWords/{file}\", \"r\").read().strip(\" \").split(\"\\n\")\n",
    "            res = []\n",
    "            for txt in corpus:\n",
    "                if \"|\" in txt:\n",
    "                    res.extend(txt.replace(\" | \", \",\").replace(\" \", \"\").split(\",\"))\n",
    "            if res != []:\n",
    "                StopWords_notNames.extend(res)\n",
    "\n",
    "    StopWords_Names = []\n",
    "    for file in os.listdir(\"StopWords/\"):\n",
    "        if file == \"StopWords_Names.txt\":\n",
    "            corpus = open(f\"StopWords/{file}\", \"r\").read().strip(\" \").split(\"\\n\")\n",
    "            for txt in corpus:\n",
    "                if \"|\" in txt:\n",
    "                    res = txt.replace(\" | \", \",\").replace(\" \", \"\").split(\",\")\n",
    "                    if res != None:\n",
    "                        StopWords_Names.append(res[0])\n",
    "\n",
    "    stop_words = []\n",
    "    for file in os.listdir(\"StopWords/\"):\n",
    "        corpus = open(f\"StopWords/{file}\", \"r\").read().strip().split(\"\\n\")\n",
    "        res = []\n",
    "        for txt in corpus:\n",
    "            if \"|\" in txt:\n",
    "                txt = txt.replace(txt, txt.split(\"|\")[0])\n",
    "                res.append(txt.strip())\n",
    "        if res != []:\n",
    "            stop_words.extend(res)\n",
    "        stop_words.extend([txt for txt in corpus if \"|\" not in txt])\n",
    "\n",
    "    stop_words.extend(StopWords_notNames)\n",
    "    stop_words.extend(StopWords_Names)\n",
    "    return stop_words\n",
    "\n",
    "\n",
    "get_stop_words()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d10e19e6-e67d-4860-92fc-21cfb5033b1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'machine replace human future work Introduction disruptive technology taking us leave it disruptive technology creates jobs depleted jobs. notice jobs disappearing jobs jobs transform humans robots machines technology creating machines replace them. Technology creates data analysis tools manipulate create custom scenarios artificial intelligence AI Big Data Machine Learning ML algorithms predict drive consumer behavior. Data Analytics tools Google Analytics today free and correctly organizations '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_stop_words(text, personalwords=True):\n",
    "    stop_words=get_stop_words()\n",
    "    if personalwords == True:\n",
    "        stop_words.extend(nltk_sw.words(\"english\"))\n",
    "    words = text.split()\n",
    "    cleaned_words = [word for word in words if word.lower() not in stop_words]\n",
    "    cleaned_text = \" \".join(cleaned_words)\n",
    "    cleaned_text = \" \".join(re.findall(\"[a-zA-Z.]+\", cleaned_text))\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "clean_stop_words(fetch_web_data(random.choice(urls)))[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8778b92e-42bb-4fe2-8d93-f1c093faff24",
   "metadata": {},
   "source": [
    "# Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fde7e1c-d74b-46ba-acb9-f684f08eaf40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 0, 0.9615384615384616, 0.6352941176470588)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_scores(text):\n",
    "    def get_subjectivity_score(text):\n",
    "        num_words = len(text.split())\n",
    "        unique_words = len(set(text.split()))\n",
    "        subjectivity_score = unique_words / num_words\n",
    "        return subjectivity_score\n",
    "\n",
    "    def get_polarity_score(text):\n",
    "        positive_words = (\n",
    "            open(\"MasterDictionary/positive-words.txt\", \"r\").read().split(\"\\n\")\n",
    "        )\n",
    "        negative_words = (\n",
    "            open(\"MasterDictionary/negative-words.txt\", \"r\").read().split(\"\\n\")\n",
    "        )\n",
    "\n",
    "        positive_count = 0\n",
    "        negative_count = 0\n",
    "\n",
    "        for word in text.split():\n",
    "            if word.lower() in positive_words:\n",
    "                positive_count += 1\n",
    "            elif word.lower() in negative_words:\n",
    "                negative_count += 1\n",
    "\n",
    "        polarity_score = (positive_count - negative_count) / (\n",
    "            positive_count + negative_count + 1\n",
    "        )\n",
    "        return polarity_score, positive_count, negative_count\n",
    "\n",
    "    subjectivity_score = get_subjectivity_score(text)\n",
    "    polarity_score, positive_count, negative_count = get_polarity_score(text)\n",
    "\n",
    "    return positive_count, negative_count, polarity_score, subjectivity_score\n",
    "\n",
    "\n",
    "get_scores(clean_stop_words(fetch_web_data(random.choice(urls))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c6d820-5482-4b94-93b1-2813a2cd759d",
   "metadata": {},
   "source": [
    "# Analysis_of_readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "165a2c03-663b-49e5-8c3e-6e96d9f2d5b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77,\n",
       " 13.73076923076923,\n",
       " 0.0718954248366013,\n",
       " 5.521065862242333,\n",
       " 13.73076923076923,\n",
       " 2.4379084967320264)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Analysis_of_readability(fetched_article):\n",
    "    sentences = fetched_article.replace(\" \", \"\").split(\".\")\n",
    "    tokens = fetched_article.split(\" \")\n",
    "    total_num_of_sentences = len(sentences)\n",
    "    total_num_of_words = len(tokens)\n",
    "\n",
    "    num_complex_words = 0\n",
    "    for token in sentences:\n",
    "        if syllables.estimate(token) > 2:\n",
    "            num_complex_words += 1\n",
    "\n",
    "    Average_Sentence_Length = total_num_of_words / total_num_of_sentences\n",
    "    Percentage_of_Complex_words = num_complex_words / total_num_of_words\n",
    "    Fog_Index = 0.4 * (Average_Sentence_Length + Percentage_of_Complex_words)\n",
    "\n",
    "    Average_Number_of_Words_Per_Sentence = total_num_of_words / total_num_of_sentences\n",
    "\n",
    "    total_syllables = sum(syllables.estimate(word) for word in sentences)\n",
    "    SYLLABLE_PER_WORD = total_syllables / total_num_of_words\n",
    "    SYLLABLE_PER_WORD\n",
    "\n",
    "    return (\n",
    "        num_complex_words,\n",
    "        Average_Sentence_Length,\n",
    "        Percentage_of_Complex_words,\n",
    "        Fog_Index,\n",
    "        Average_Number_of_Words_Per_Sentence,\n",
    "        SYLLABLE_PER_WORD,\n",
    "    )\n",
    "\n",
    "\n",
    "Analysis_of_readability(clean_stop_words(fetch_web_data(random.choice(urls))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb50f10-e36e-4725-9fd9-b970f19bf719",
   "metadata": {},
   "source": [
    "# get_personal_pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e937ae7b-0704-4872-b111-725d752ec6f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 6.93010752688172)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_personal_pronouns(tokens):\n",
    "    personal_pronouns = [\n",
    "        \"I\",\n",
    "        \"me\",\n",
    "        \"my\",\n",
    "        \"mine\",\n",
    "        \"you\",\n",
    "        \"your\",\n",
    "        \"yours\",\n",
    "        \"he\",\n",
    "        \"him\",\n",
    "        \"his\",\n",
    "        \"she\",\n",
    "        \"her\",\n",
    "        \"hers\",\n",
    "        \"it\",\n",
    "        \"its\",\n",
    "        \"we\",\n",
    "        \"us\",\n",
    "        \"our\",\n",
    "        \"ours\",\n",
    "        \"they\",\n",
    "        \"them\",\n",
    "        \"their\",\n",
    "        \"theirs\",\n",
    "    ]\n",
    "    num_personal_pronouns = sum(\n",
    "        [1 for word in tokens if word.lower() in personal_pronouns]\n",
    "    )\n",
    "\n",
    "    total_chars = sum(len(word) for word in tokens)\n",
    "    avg_word_length = total_chars / len(tokens)\n",
    "\n",
    "    return num_personal_pronouns, avg_word_length\n",
    "\n",
    "\n",
    "corpus = clean_stop_words(fetch_web_data(random.choice(urls)),personalwords=False)\n",
    "\n",
    "res = re.findall(\"[A-Za-z]+\", corpus)\n",
    "get_personal_pronouns(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e566d6-21d9-468d-a7ad-97faf3fe280e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T08:09:17.329442Z",
     "iopub.status.busy": "2023-05-02T08:09:17.329442Z",
     "iopub.status.idle": "2023-05-02T08:09:17.361058Z",
     "shell.execute_reply": "2023-05-02T08:09:17.361058Z",
     "shell.execute_reply.started": "2023-05-02T08:09:17.329442Z"
    },
    "tags": []
   },
   "source": [
    "<!-- # 1. All input variables in “Input.xlsx” - columns\n",
    "# 2. POSITIVE SCORE - pos_score\n",
    "# 3. NEGATIVE SCORE - neg_score\n",
    "# 4. POLARITY SCORE - Polarity_Score\n",
    "# 5. SUBJECTIVITY SCORE - Subjectivity_Score\n",
    "# 6. AVG SENTENCE LENGTH - Average_Sentence_Length\n",
    "# 7. PERCENTAGE OF COMPLEX WORDS - Percentage_of_Complex_words\n",
    "# 8. FOG INDEX - Fog_Index\n",
    "# 9. AVG NUMBER OF WORDS PER SENTENCE - Average_Number_of_Words_Per_Sentence\n",
    "# 10. COMPLEX WORD COUNT - num_complex_words\n",
    "# 11. WORD COUNT - total_num_of_words\n",
    "# 12. SYLLABLE PER WORD - SYLLABLE_PER_WORD\n",
    "# 13. PERSONAL PRONOUNS - num_personal_pronouns\n",
    "# 14. AVG WORD LENGTH - avg_word_length -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac9a54e-fe67-401e-a011-35bb60dc80db",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02c91fad-5b97-4dae-a792-ce2f3b80a635",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/ Not Found....!\n",
      "Page https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/ Not Found....!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>URL_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>123.0</th>\n",
       "      <td>75</td>\n",
       "      <td>21</td>\n",
       "      <td>0.556701</td>\n",
       "      <td>0.398870</td>\n",
       "      <td>18.896552</td>\n",
       "      <td>0.052311</td>\n",
       "      <td>7.579545</td>\n",
       "      <td>18.896552</td>\n",
       "      <td>86</td>\n",
       "      <td>7746</td>\n",
       "      <td>2.003041</td>\n",
       "      <td>2</td>\n",
       "      <td>7.639233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321.0</th>\n",
       "      <td>38</td>\n",
       "      <td>12</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>0.534426</td>\n",
       "      <td>24.760000</td>\n",
       "      <td>0.040388</td>\n",
       "      <td>9.920155</td>\n",
       "      <td>24.760000</td>\n",
       "      <td>25</td>\n",
       "      <td>2675</td>\n",
       "      <td>1.854604</td>\n",
       "      <td>1</td>\n",
       "      <td>7.695082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345.0</th>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.649266</td>\n",
       "      <td>15.554054</td>\n",
       "      <td>0.063423</td>\n",
       "      <td>6.246991</td>\n",
       "      <td>15.554054</td>\n",
       "      <td>73</td>\n",
       "      <td>5022</td>\n",
       "      <td>1.771503</td>\n",
       "      <td>0</td>\n",
       "      <td>7.168595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4321.0</th>\n",
       "      <td>34</td>\n",
       "      <td>26</td>\n",
       "      <td>0.131148</td>\n",
       "      <td>0.670504</td>\n",
       "      <td>20.576271</td>\n",
       "      <td>0.048600</td>\n",
       "      <td>8.249948</td>\n",
       "      <td>20.576271</td>\n",
       "      <td>59</td>\n",
       "      <td>5702</td>\n",
       "      <td>1.894563</td>\n",
       "      <td>1</td>\n",
       "      <td>7.153179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432.0</th>\n",
       "      <td>34</td>\n",
       "      <td>26</td>\n",
       "      <td>0.131148</td>\n",
       "      <td>0.670504</td>\n",
       "      <td>20.576271</td>\n",
       "      <td>0.048600</td>\n",
       "      <td>8.249948</td>\n",
       "      <td>20.576271</td>\n",
       "      <td>59</td>\n",
       "      <td>5702</td>\n",
       "      <td>1.894563</td>\n",
       "      <td>1</td>\n",
       "      <td>7.153179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50921.0</th>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>0.729592</td>\n",
       "      <td>19.363636</td>\n",
       "      <td>0.050078</td>\n",
       "      <td>7.765486</td>\n",
       "      <td>19.363636</td>\n",
       "      <td>32</td>\n",
       "      <td>3024</td>\n",
       "      <td>1.902973</td>\n",
       "      <td>0</td>\n",
       "      <td>6.738342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51382.8</th>\n",
       "      <td>24</td>\n",
       "      <td>59</td>\n",
       "      <td>-0.416667</td>\n",
       "      <td>0.532274</td>\n",
       "      <td>30.450980</td>\n",
       "      <td>0.032840</td>\n",
       "      <td>12.193528</td>\n",
       "      <td>30.450980</td>\n",
       "      <td>51</td>\n",
       "      <td>7526</td>\n",
       "      <td>1.912428</td>\n",
       "      <td>0</td>\n",
       "      <td>6.444223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51844.6</th>\n",
       "      <td>81</td>\n",
       "      <td>28</td>\n",
       "      <td>0.481818</td>\n",
       "      <td>0.698492</td>\n",
       "      <td>23.972222</td>\n",
       "      <td>0.041136</td>\n",
       "      <td>9.605343</td>\n",
       "      <td>23.972222</td>\n",
       "      <td>71</td>\n",
       "      <td>7762</td>\n",
       "      <td>1.792584</td>\n",
       "      <td>10</td>\n",
       "      <td>6.723896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52306.4</th>\n",
       "      <td>32</td>\n",
       "      <td>22</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.620419</td>\n",
       "      <td>19.921875</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>7.987574</td>\n",
       "      <td>19.921875</td>\n",
       "      <td>60</td>\n",
       "      <td>5687</td>\n",
       "      <td>1.879216</td>\n",
       "      <td>7</td>\n",
       "      <td>6.345953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52768.2</th>\n",
       "      <td>51</td>\n",
       "      <td>30</td>\n",
       "      <td>0.256098</td>\n",
       "      <td>0.766436</td>\n",
       "      <td>22.238095</td>\n",
       "      <td>0.044968</td>\n",
       "      <td>8.913225</td>\n",
       "      <td>22.238095</td>\n",
       "      <td>42</td>\n",
       "      <td>5016</td>\n",
       "      <td>2.175589</td>\n",
       "      <td>2</td>\n",
       "      <td>7.595855</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "URL_ID                                                                        \n",
       "123.0                75              21        0.556701            0.398870   \n",
       "321.0                38              12        0.509804            0.534426   \n",
       "2345.0               22              20        0.046512            0.649266   \n",
       "4321.0               34              26        0.131148            0.670504   \n",
       "432.0                34              26        0.131148            0.670504   \n",
       "...                 ...             ...             ...                 ...   \n",
       "50921.0               5              27       -0.666667            0.729592   \n",
       "51382.8              24              59       -0.416667            0.532274   \n",
       "51844.6              81              28        0.481818            0.698492   \n",
       "52306.4              32              22        0.181818            0.620419   \n",
       "52768.2              51              30        0.256098            0.766436   \n",
       "\n",
       "         AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "URL_ID                                                                 \n",
       "123.0              18.896552                     0.052311   7.579545   \n",
       "321.0              24.760000                     0.040388   9.920155   \n",
       "2345.0             15.554054                     0.063423   6.246991   \n",
       "4321.0             20.576271                     0.048600   8.249948   \n",
       "432.0              20.576271                     0.048600   8.249948   \n",
       "...                      ...                          ...        ...   \n",
       "50921.0            19.363636                     0.050078   7.765486   \n",
       "51382.8            30.450980                     0.032840  12.193528   \n",
       "51844.6            23.972222                     0.041136   9.605343   \n",
       "52306.4            19.921875                     0.047059   7.987574   \n",
       "52768.2            22.238095                     0.044968   8.913225   \n",
       "\n",
       "         AVG NUMBER OF WORDS PER SENTENCE   COMPLEX WORD COUNT  WORD COUNT   \\\n",
       "URL_ID                                                                        \n",
       "123.0                           18.896552                   86         7746   \n",
       "321.0                           24.760000                   25         2675   \n",
       "2345.0                          15.554054                   73         5022   \n",
       "4321.0                          20.576271                   59         5702   \n",
       "432.0                           20.576271                   59         5702   \n",
       "...                                   ...                  ...          ...   \n",
       "50921.0                         19.363636                   32         3024   \n",
       "51382.8                         30.450980                   51         7526   \n",
       "51844.6                         23.972222                   71         7762   \n",
       "52306.4                         19.921875                   60         5687   \n",
       "52768.2                         22.238095                   42         5016   \n",
       "\n",
       "         SYLLABLE PER WORD   PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "URL_ID                                                           \n",
       "123.0              2.003041                  2         7.639233  \n",
       "321.0              1.854604                  1         7.695082  \n",
       "2345.0             1.771503                  0         7.168595  \n",
       "4321.0             1.894563                  1         7.153179  \n",
       "432.0              1.894563                  1         7.153179  \n",
       "...                     ...                ...              ...  \n",
       "50921.0            1.902973                  0         6.738342  \n",
       "51382.8            1.912428                  0         6.444223  \n",
       "51844.6            1.792584                 10         6.723896  \n",
       "52306.4            1.879216                  7         6.345953  \n",
       "52768.2            2.175589                  2         7.595855  \n",
       "\n",
       "[112 rows x 13 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_r = []\n",
    "url_r = []\n",
    "pos_score_r = []\n",
    "neg_score_r = []\n",
    "Polarity_Score_r = []\n",
    "Polarity_Score_r = []\n",
    "Subjectivity_Score_r = []\n",
    "Average_Sentence_Length_r = []\n",
    "Percentage_of_Complex_words_r = []\n",
    "Fog_Index_r = []\n",
    "Average_Number_of_Words_Per_Sentence_r = []\n",
    "num_complex_words_r = []\n",
    "total_num_of_words_r = []\n",
    "SYLLABLE_PER_WORD_r = []\n",
    "num_personal_pronouns_r = []\n",
    "avg_word_length_r = []\n",
    "\n",
    "\n",
    "# Iterating through URLS\n",
    "for n in range(len(urls)):\n",
    "    try:\n",
    "        # fetch_web_data\n",
    "        fetched_article = fetch_web_data(urls[n])\n",
    "    except:\n",
    "        print(f\"Page {urls[n]} Not Found....!\")\n",
    "        continue\n",
    "    index = df.iloc[n]\n",
    "    id_ = index[0]\n",
    "    url_ = index[1]\n",
    "\n",
    "    # clean_stop_words\n",
    "    tokens = clean_stop_words(fetched_article)\n",
    "    total_num_of_words = len(tokens)\n",
    "\n",
    "    pos_score, neg_score, Polarity_Score, Subjectivity_Score = get_scores(tokens)\n",
    "\n",
    "    (\n",
    "        num_complex_words,\n",
    "        Average_Sentence_Length,\n",
    "        Percentage_of_Complex_words,\n",
    "        Fog_Index,\n",
    "        Average_Number_of_Words_Per_Sentence,\n",
    "        SYLLABLE_PER_WORD,\n",
    "    ) = Analysis_of_readability(fetched_article)\n",
    "\n",
    "    tmp=clean_stop_words(fetched_article,personalwords=False)\n",
    "    res = re.findall(\"[A-Za-z]+\", tmp)\n",
    "    num_personal_pronouns, avg_word_length = get_personal_pronouns(res)\n",
    "\n",
    "    # Appending obtained variables into respective lists\n",
    "    id_r.append(id_)\n",
    "    url_r.append(url_)\n",
    "    pos_score_r.append(pos_score)\n",
    "    neg_score_r.append(neg_score)\n",
    "    Polarity_Score_r.append(Polarity_Score)\n",
    "    Subjectivity_Score_r.append(Subjectivity_Score)\n",
    "    Average_Sentence_Length_r.append(Average_Sentence_Length)\n",
    "    Percentage_of_Complex_words_r.append(Percentage_of_Complex_words)\n",
    "    Fog_Index_r.append(Fog_Index)\n",
    "    Average_Number_of_Words_Per_Sentence_r.append(Average_Number_of_Words_Per_Sentence)\n",
    "    num_complex_words_r.append(num_complex_words)\n",
    "    total_num_of_words_r.append(total_num_of_words)\n",
    "    SYLLABLE_PER_WORD_r.append(SYLLABLE_PER_WORD)\n",
    "    num_personal_pronouns_r.append(num_personal_pronouns)\n",
    "    avg_word_length_r.append(avg_word_length)\n",
    "\n",
    "\n",
    "output = {\n",
    "    \"URL_ID\": id_r,\n",
    "    \"POSITIVE SCORE\": pos_score_r,\n",
    "    \"NEGATIVE SCORE\": neg_score_r,\n",
    "    \"POLARITY SCORE\": Polarity_Score_r,\n",
    "    \"SUBJECTIVITY SCORE\": Subjectivity_Score_r,\n",
    "    \"AVG SENTENCE LENGTH\": Average_Sentence_Length_r,\n",
    "    \"PERCENTAGE OF COMPLEX WORDS\": Percentage_of_Complex_words_r,\n",
    "    \"FOG INDEX\": Fog_Index_r,\n",
    "    \"AVG NUMBER OF WORDS PER SENTENCE\": Average_Number_of_Words_Per_Sentence_r,\n",
    "    \" COMPLEX WORD COUNT\": num_complex_words_r,\n",
    "    \"WORD COUNT \": total_num_of_words_r,\n",
    "    \"SYLLABLE PER WORD \": SYLLABLE_PER_WORD_r,\n",
    "    \"PERSONAL PRONOUNS\": num_personal_pronouns_r,\n",
    "    \"AVG WORD LENGTH\": avg_word_length_r,\n",
    "}\n",
    "\n",
    "output_df = pd.DataFrame(output).set_index(\"URL_ID\")\n",
    "output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0111265e-aeb0-44f6-b20d-ca2207b1cba5",
   "metadata": {},
   "source": [
    "# Exporting the output_df to excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f4cf4fb-5b76-4933-88e3-3c6c07f5b33f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_df.to_excel(\"Output Data.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
